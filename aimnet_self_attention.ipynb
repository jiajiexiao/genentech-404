{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "\n",
    "# from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Optional\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genentech404Dataset(utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file: str,\n",
    "        is_test: bool = False,\n",
    "        transform=StandardScaler(),\n",
    "        imputer=SimpleImputer(strategy=\"median\"),\n",
    "    ) -> None:\n",
    "        self.df = pd.read_csv(csv_file).set_index([\"RID_HASH\"])\n",
    "        # self.df = self.df.astype(\n",
    "        #     {\n",
    "        #         \"VISCODE\": int,\n",
    "        #         \"AGE\": float,\n",
    "        #         \"PTGENDER_num\": pd.api.types.CategoricalDtype([0, 1]),\n",
    "        #         \"PTEDUCAT\": int,\n",
    "        #         \"DX_num\": pd.api.types.CategoricalDtype([0, 1, 2]),\n",
    "        #         \"APOE4\": float,\n",
    "        #         \"CDRSB\": float,\n",
    "        #         \"MMSE\": float,\n",
    "        #         \"ADAS13\": float,\n",
    "        #         \"Ventricles\": float,\n",
    "        #         \"Hippocampus\": float,\n",
    "        #         \"WholeBrain\": float,\n",
    "        #         \"Entorhinal\": float,\n",
    "        #         \"Fusiform\": float,\n",
    "        #         \"MidTemp\": float,\n",
    "        #     }\n",
    "        # )\n",
    "        self.mask_categorical = np.array(\n",
    "            [\n",
    "                False,\n",
    "                False,\n",
    "                True,\n",
    "                False,\n",
    "                True,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "                False,\n",
    "            ]\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.imputer = imputer\n",
    "        if is_test:\n",
    "            self.df.loc[:, ~self.mask_categorical] = self.transform.transform(\n",
    "                self.df.loc[:, ~self.mask_categorical]\n",
    "            )\n",
    "            self.df.loc[:, :] = self.imputer.transform(self.df.values)\n",
    "        else:\n",
    "            self.df.loc[:, ~self.mask_categorical] = self.transform.fit_transform(\n",
    "                self.df.loc[:, ~self.mask_categorical]\n",
    "            )\n",
    "            self.imputer.fit(self.df.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.df.iloc[idx, :].values.astype(np.float32)\n",
    "\n",
    "\n",
    "train_dataset = Genentech404Dataset(\"genentech-404-challenge/dev_set.csv\")\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=500)\n",
    "mask_categorical = train_dataset.mask_categorical\n",
    "\n",
    "test_a_dataset = Genentech404Dataset(\n",
    "    \"genentech-404-challenge/test_A_inferred.csv\",\n",
    "    is_test=True,\n",
    "    transform=train_dataset.transform,\n",
    "    imputer=train_dataset.imputer,\n",
    ")\n",
    "test_a_loader = utils.data.DataLoader(test_a_dataset, batch_size=500)\n",
    "test_b_dataset = Genentech404Dataset(\n",
    "    \"genentech-404-challenge/test_B_inferred.csv\",\n",
    "    is_test=True,\n",
    "    transform=train_dataset.transform,\n",
    "    imputer=train_dataset.imputer,\n",
    ")\n",
    "test_b_loader = utils.data.DataLoader(test_b_dataset, batch_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LitAutoEncoder(pl.LightningModule):\n",
    "#     def __init__(self, encoder, decoder):\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # training_step defines the train loop.\n",
    "#         # it is independent of forward\n",
    "#         x = batch\n",
    "#         x = x.view(x.size(0), -1)\n",
    "\n",
    "#         z = self.encoder(x)\n",
    "#         x_hat = self.decoder(z)\n",
    "#         loss = nn.functional.mse_loss(x_hat, x)\n",
    "#         # Logging to TensorBoard by default\n",
    "#         self.log(\"train_loss\", loss)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "#         return optimizer\n",
    "\n",
    "\n",
    "# encoder = nn.Sequential(nn.Linear(15 * 1, 5), nn.ReLU(), nn.Linear(5, 2))\n",
    "# decoder = nn.Sequential(nn.Linear(2, 5), nn.ReLU(), nn.Linear(5, 15 * 1))\n",
    "# autoencoder = LitAutoEncoder(encoder, decoder)\n",
    "# trainer = pl.Trainer(limit_train_batches=100, max_epochs=3)\n",
    "# trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AimNetSelfAttention(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_continuous,\n",
    "        encoder_categorical,\n",
    "        decoder_continuous,\n",
    "        decoder_categorical,\n",
    "        pooling,\n",
    "        alpha: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder_continuous = encoder_continuous\n",
    "        self.encoder_categorical = encoder_categorical\n",
    "        self.decoder_continuous = decoder_continuous\n",
    "        self.decoder_categorical = decoder_categorical\n",
    "        self.pooling = pooling\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch\n",
    "        x = x.view(x.size(0), x.size(1), 1)\n",
    "        x_categorical = x[:, mask_categorical, :]\n",
    "        x_continuous = x[:, ~mask_categorical, :]\n",
    "        z_categorical = self.encoder_categorical(x_categorical)\n",
    "        z_continuous = self.encoder_continuous(x_continuous)\n",
    "\n",
    "        z = torch.cat((z_categorical, z_continuous), 1)\n",
    "        z, _ = self.pooling(z, z, z)\n",
    "        z_categorical = z[:, mask_categorical, :]\n",
    "        z_continuous = z[:, ~mask_categorical, :]\n",
    "\n",
    "        x_continuous_hat = self.decoder_continuous(z_continuous)\n",
    "        x_categorical_hat = self.decoder_categorical(z_categorical, x_categorical)\n",
    "        batch[:, mask_categorical] = torch.stack([torch.argmax(ele, dim=1) for ele in x_categorical_hat], dim=1).to(torch.float)\n",
    "        batch[:, ~mask_categorical] = x_continuous_hat.view(x_continuous_hat.size(0), x_continuous_hat.size(1))\n",
    "        return batch, x_continuous_hat, x_categorical_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x = x.view(x.size(0), x.size(1), 1)\n",
    "        x_categorical = x[:, mask_categorical, :]\n",
    "        x_continuous = x[:, ~mask_categorical, :]\n",
    "        # z_categorical = self.encoder_categorical(x_categorical)\n",
    "        # z_continuous = self.encoder_continuous(x_continuous)\n",
    "\n",
    "        # z = torch.cat((z_categorical, z_continuous), 1)\n",
    "        # z, _ = self.pooling(z, z, z)\n",
    "        # z_categorical = z[:, mask_categorical, :]\n",
    "        # z_continuous = z[:, ~mask_categorical, :]\n",
    "\n",
    "\n",
    "        # x_continuous_hat = self.decoder_continuous(z_continuous)\n",
    "        # x_categorical_hat = self.decoder_categorical(z_categorical, x_categorical)\n",
    "        loss_ce = 0\n",
    "        pred, x_continuous_hat, x_categorical_hat = self.forward(batch)\n",
    "\n",
    "        for i, _ in enumerate(self.decoder_categorical.transforms):\n",
    "            loss_ce += nn.functional.cross_entropy(x_categorical_hat[i], x_categorical[:, i, :].squeeze().to(torch.long))\n",
    "\n",
    "        loss_mse = nn.functional.mse_loss(\n",
    "            x_continuous_hat, x_continuous, reduction=\"mean\"\n",
    "        )\n",
    "        loss = self.alpha * loss_ce + loss_mse\n",
    "\n",
    "        self.log(\"train_loss\", {\"loss\": loss, \"loss_ce\": loss_ce, \"loss_mse\": loss_mse})\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "k = 20\n",
    "p_dropout = 0.15\n",
    "encoder_continuous = nn.Sequential(\n",
    "    nn.Linear(1, k), nn.ReLU(), nn.Dropout1d(p_dropout), nn.Linear(k, k)\n",
    ")\n",
    "\n",
    "\n",
    "class CategoricalEmbedding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embed = nn.ModuleList([nn.Embedding(2, k), nn.Embedding(3, k)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.concat(\n",
    "            [embed(x[:, i, :].to(torch.int)) for i, embed in enumerate(self.embed)],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "\n",
    "categorical_embedding = CategoricalEmbedding()\n",
    "encoder_categorical = nn.Sequential(\n",
    "    categorical_embedding,\n",
    "    nn.Dropout1d(p_dropout),\n",
    ")\n",
    "\n",
    "pooling = nn.MultiheadAttention(k, 1)\n",
    "decoder_continuous = nn.Sequential(nn.ReLU(), nn.Linear(k, 1))\n",
    "# decoder_continuous = nn.Linear(k, 1)\n",
    "\n",
    "\n",
    "class DecoderCategorical(nn.Module):\n",
    "    def __init__(self, embedding, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.transforms = nn.ModuleList()\n",
    "        for embed in self.embedding.embed:\n",
    "            self.transforms.append(\n",
    "                nn.Sequential(nn.Linear(k, embed.num_embeddings), nn.ReLU(), nn.Softmax(dim))\n",
    "            )\n",
    "            \n",
    "\n",
    "    def forward(self, context_embedding, categorical_values):\n",
    "        categorical_embeddings = self.embedding(categorical_values)\n",
    "        out = []\n",
    "        for i, module in enumerate(self.transforms):\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "            out.append(module(categorical_embeddings[:, i, :]))\n",
    "        return out\n",
    "\n",
    "decoder_categorical = DecoderCategorical(categorical_embedding, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | encoder_continuous  | Sequential         | 460   \n",
      "1 | encoder_categorical | Sequential         | 100   \n",
      "2 | decoder_continuous  | Sequential         | 21    \n",
      "3 | decoder_categorical | DecoderCategorical | 205   \n",
      "4 | pooling             | MultiheadAttention | 1.7 K \n",
      "-----------------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 9/9 [00:00<00:00, 11.09it/s, loss=0.647, v_num=44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 9/9 [00:00<00:00, 10.88it/s, loss=0.647, v_num=44]\n"
     ]
    }
   ],
   "source": [
    "aimnet = AimNetSelfAttention(\n",
    "    encoder_continuous,\n",
    "    encoder_categorical,\n",
    "    decoder_continuous,\n",
    "    decoder_categorical,\n",
    "    pooling,\n",
    "    0.2,\n",
    ")\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=150, log_every_n_steps=25) #,fast_dev_run=True)\n",
    "trainer.fit(model=aimnet, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | encoder_continuous  | Sequential         | 460   \n",
      "1 | encoder_categorical | Sequential         | 100   \n",
      "2 | decoder_continuous  | Sequential         | 21    \n",
      "3 | decoder_categorical | DecoderCategorical | 205   \n",
      "4 | pooling             | MultiheadAttention | 1.7 K \n",
      "-----------------------------------------------------------\n",
      "2.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 6/6 [00:00<00:00,  6.19it/s, loss=0.571, v_num=45]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 6/6 [00:00<00:00,  6.14it/s, loss=0.571, v_num=45]\n"
     ]
    }
   ],
   "source": [
    "finetuner = pl.Trainer(\n",
    "    limit_train_batches=100, max_epochs=10, log_every_n_steps=25\n",
    ")  # ,fast_dev_run=True)\n",
    "finetuner.fit(\n",
    "    model=aimnet,\n",
    "    train_dataloaders=utils.data.DataLoader(\n",
    "        utils.data.ConcatDataset([test_a_dataset, test_b_dataset]),\n",
    "        batch_size=500,\n",
    "    ), \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 15.61it/s]\n"
     ]
    }
   ],
   "source": [
    "results = finetuner.predict(model=aimnet, dataloaders=test_a_loader)\n",
    "predicts = torch.concat([result[0] for result in results], dim=0).numpy()\n",
    "predicts[:, ~mask_categorical] = train_dataset.transform.inverse_transform(predicts[:, ~mask_categorical])\n",
    "\n",
    "df_test_a = pd.read_csv(\"genentech-404-challenge/test_A.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_a_inferred = pd.read_csv(\"genentech-404-challenge/test_A_inferred.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_a_pred = pd.DataFrame(predicts, columns=df_test_a_inferred.columns, index=df_test_a_inferred.index)\n",
    "\n",
    "mask = (~df_test_a_inferred.isna()) & (df_test_a.isna())\n",
    "df_test_a_pred[mask] = df_test_a_inferred[mask]\n",
    "df_test_a_pred.VISCODE = df_test_a.VISCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00,  8.25it/s]\n"
     ]
    }
   ],
   "source": [
    "results = finetuner.predict(model=aimnet, dataloaders=test_b_loader)\n",
    "predicts = torch.concat([result[0] for result in results], dim=0).numpy()\n",
    "predicts[:, ~mask_categorical] = train_dataset.transform.inverse_transform(predicts[:, ~mask_categorical])\n",
    "\n",
    "df_test_b = pd.read_csv(\"genentech-404-challenge/test_B.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_b_inferred = pd.read_csv(\"genentech-404-challenge/test_B_inferred.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_b_pred = pd.DataFrame(predicts, columns=df_test_b_inferred.columns, index=df_test_b_inferred.index)\n",
    "\n",
    "mask = (~df_test_b_inferred.isna()) & (df_test_b.isna())\n",
    "df_test_b_pred[mask] = df_test_b_inferred[mask]\n",
    "df_test_b_pred.VISCODE = df_test_b.VISCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_df(\n",
    "    ref_df: pd.DataFrame, df_pred_test_a: pd.DataFrame, df_pred_test_b: pd.DataFrame\n",
    "):\n",
    "    df_submit = []\n",
    "    for df, test_type in zip([df_pred_test_a, df_pred_test_b], [\"test_A\", \"test_B\"]):\n",
    "        df_new = pd.melt(\n",
    "            df,\n",
    "            id_vars=[\"RID_HASH\", \"VISCODE\"],\n",
    "            value_vars=[\n",
    "                \"AGE\",\n",
    "                \"PTGENDER_num\",\n",
    "                \"PTEDUCAT\",\n",
    "                \"DX_num\",\n",
    "                \"APOE4\",\n",
    "                \"CDRSB\",\n",
    "                \"MMSE\",\n",
    "                \"ADAS13\",\n",
    "                \"Ventricles\",\n",
    "                \"Hippocampus\",\n",
    "                \"WholeBrain\",\n",
    "                \"Entorhinal\",\n",
    "                \"Fusiform\",\n",
    "                \"MidTemp\",\n",
    "            ],\n",
    "        ).rename(columns={\"value\": \"Predicted\"})\n",
    "        df_new[\"Id\"] = df_new.apply(\n",
    "            lambda x: f\"{x['RID_HASH']}_{x['VISCODE']}_{x['variable']}_{test_type}\",\n",
    "            axis=1,\n",
    "        )\n",
    "        df_submit.append(df_new[[\"Id\", \"Predicted\"]])\n",
    "    df_submit = pd.concat(df_submit).set_index(\"Id\")\n",
    "    return df_submit.loc[ref_df[\"Id\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_sub = pd.read_csv(\"genentech-404-challenge/sample_submission.csv\")\n",
    "df_submission = get_submission_df(\n",
    "    df_sample_sub,\n",
    "    df_test_a_pred.reset_index(),\n",
    "    df_test_b_pred.reset_index(),\n",
    ")\n",
    "df_submission.to_csv(\"attention_dropout_11092022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle_genentech-404-challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b1b4e6b55d601fd1eb4b734b7c7b613a61fa9d31c944763ca84ddf1bd43ea2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
