{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "\n",
    "# from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Optional\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9906,) (2397,)\n"
     ]
    }
   ],
   "source": [
    "df_dev_set = pd.read_csv(\"genentech-404-challenge/dev_set.csv\").set_index(\"RID_HASH\")\n",
    "df_dev_1 = pd.read_csv(\"genentech-404-challenge/dev_1.csv\").set_index(\"RID_HASH\")\n",
    "df_dev_2 = pd.read_csv(\"genentech-404-challenge/dev_2.csv\").set_index(\"RID_HASH\")\n",
    "df_dev_3 = pd.read_csv(\"genentech-404-challenge/dev_3.csv\").set_index(\"RID_HASH\")\n",
    "df_test_a = pd.read_csv(\"genentech-404-challenge/test_A.csv\").set_index(\"RID_HASH\")\n",
    "df_test_b = pd.read_csv(\"genentech-404-challenge/test_B.csv\").set_index(\"RID_HASH\")\n",
    "df_test_a_inferred = pd.read_csv(\"genentech-404-challenge/test_A_inferred.csv\").set_index(\"RID_HASH\")\n",
    "df_test_b_inferred = pd.read_csv(\"genentech-404-challenge/test_B_inferred.csv\").set_index(\"RID_HASH\")\n",
    "\n",
    "mask_categorical = np.array(\n",
    "    [\n",
    "        False,\n",
    "        False,\n",
    "        True,\n",
    "        False,\n",
    "        True,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "    ]\n",
    ")\n",
    "scaler = StandardScaler().fit(df_dev_set.loc[:, ~mask_categorical])\n",
    "std = df_dev_set.std().values\n",
    "\n",
    "df_dev123 = pd.concat([df_dev_1, df_dev_2, df_dev_3])\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=0)\n",
    "for train_idx, val_idx in gss.split(range(df_dev123.shape[0]), groups=df_dev123.index.values):\n",
    "    print(train_idx.shape, val_idx.shape)\n",
    "df_train = df_dev123.iloc[train_idx,:]\n",
    "df_train_target = pd.concat([df_dev_set, df_dev_set, df_dev_set]).iloc[train_idx, :]\n",
    "df_val = df_dev123.iloc[val_idx,:]\n",
    "df_val_target = pd.concat([df_dev_set, df_dev_set, df_dev_set]).iloc[val_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genentech404Dataset(utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_input: pd.DataFrame,\n",
    "        mask_categorical: np.ndarray,\n",
    "        df_target: Optional[pd.DataFrame] = None,\n",
    "        transform=StandardScaler(),\n",
    "        self_training: bool = False,\n",
    "    ) -> None:\n",
    "        continous_columns = df_input.columns[~mask_categorical]\n",
    "        self.df_input = df_input.reset_index()\n",
    "        self.df_target = (\n",
    "            df_target.reset_index() if df_target is not None else self.df_input.copy()\n",
    "        )\n",
    "        self.self_training = self_training  # this is for whether to apply dropout\n",
    "        self.mask_categorical = mask_categorical\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df_input.loc[:, continous_columns] = self.transform.transform(\n",
    "            self.df_input.loc[:, continous_columns]\n",
    "        )\n",
    "        self.df_input = self.df_input.fillna(\n",
    "            0\n",
    "        )  # fill missing values to mimic dropout in training\n",
    "\n",
    "        if df_target is not None:\n",
    "            self.df_target.loc[:, continous_columns] = self.transform.transform(\n",
    "                self.df_target.loc[:, continous_columns]\n",
    "            )\n",
    "        else:\n",
    "            self.df_target = self.df_input.copy()  # this is just for test case\n",
    "        self.df_input.set_index(\"RID_HASH\", inplace=True)\n",
    "        self.df_target.set_index(\"RID_HASH\", inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_input.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (\n",
    "            self.df_input.iloc[idx, :].values.astype(np.float32),\n",
    "            self.df_target.iloc[idx, :].values.astype(np.float32),\n",
    "            self.self_training,\n",
    "        )\n",
    "\n",
    "\n",
    "self_train_dataset = Genentech404Dataset(\n",
    "    df_dev_set, mask_categorical, df_dev_set, scaler, self_training=True\n",
    ")\n",
    "self_train_loader = utils.data.DataLoader(self_train_dataset, batch_size=500)\n",
    "\n",
    "supervised_train_dataset = Genentech404Dataset(\n",
    "    df_train, mask_categorical, df_train_target, scaler, self_training=False\n",
    ")\n",
    "supervised_train_loader = utils.data.DataLoader(self_train_dataset, batch_size=500)\n",
    "train_loader = utils.data.DataLoader(\n",
    "        utils.data.ConcatDataset([supervised_train_dataset, self_train_dataset]),\n",
    "        batch_size=500,\n",
    "        shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataset = Genentech404Dataset(\n",
    "    df_val, mask_categorical, df_val_target, scaler, self_training=False\n",
    ")\n",
    "val_loader = utils.data.DataLoader(val_dataset, batch_size=500)\n",
    "\n",
    "\n",
    "test_a_dataset = Genentech404Dataset(\n",
    "    df_test_a_inferred,\n",
    "    mask_categorical,\n",
    "    None,\n",
    "    scaler,\n",
    "    self_training=False,\n",
    ")\n",
    "test_a_loader = utils.data.DataLoader(test_a_dataset, batch_size=500)\n",
    "test_b_dataset = Genentech404Dataset(\n",
    "    df_test_b_inferred,\n",
    "    mask_categorical,\n",
    "    None,\n",
    "    scaler,\n",
    "    self_training=False,\n",
    ")\n",
    "test_b_loader = utils.data.DataLoader(test_b_dataset, batch_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 227   \n",
      "1 | decoder | Sequential | 240   \n",
      "---------------------------------------\n",
      "467       Trainable params\n",
      "0         Non-trainable params\n",
      "467       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 34/34 [00:04<00:00,  6.91it/s, loss=0.579, v_num=58]\n"
     ]
    }
   ],
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, std, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.p_dropout = p_dropout\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, y, self_train = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.stack(\n",
    "            [\n",
    "                nn.functional.dropout(\n",
    "                    x[i, :], p=self.p_dropout, training=self_train[i].item()\n",
    "                )\n",
    "                for i in range(self_train.size(0))\n",
    "            ]\n",
    "        )\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return y, x_hat, z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y, x_hat, z = self.forward(batch)\n",
    "        loss = nn.functional.mse_loss(x_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        nmae_loss = (nn.functional.l1_loss(x_hat, y, reduction='none').detach().numpy()/self.std).mean()\n",
    "        self.log(\"train_nmae\", nmae_loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y, x_hat, z = self.forward(batch)\n",
    "        loss = nn.functional.mse_loss(x_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        nmae_loss = (nn.functional.l1_loss(x_hat, y, reduction='none').detach().numpy()/self.std).mean()\n",
    "        self.log(\"val_nmae\", nmae_loss)\n",
    "        return loss        \n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(15, 10),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.SELU(),\n",
    "    nn.Linear(10, 5),\n",
    "    nn.SELU(),\n",
    "    nn.Linear(5, 2),\n",
    ")\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(2, 5), nn.SELU(), nn.Linear(5, 10), nn.SELU(), nn.Linear(10, 15)\n",
    ")\n",
    "# encoder = nn.Sequential(nn.Dropout1d(p_dropout), nn.Linear(15 * 1, 5), nn.ReLU(), nn.Linear(5, 2))\n",
    "# decoder = nn.Sequential(nn.Linear(2, 5), nn.ReLU(), nn.Linear(5, 15 * 1))\n",
    "autoencoder = LitAutoEncoder(encoder, decoder, std, 0.1)\n",
    "trainer = pl.Trainer(\n",
    "    limit_train_batches=100, max_epochs=250, log_every_n_steps=25,  check_val_every_n_epoch=1, \n",
    "    callbacks=[pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_nmae\", mode=\"min\", patience=10, verbose=False)]\n",
    ")  # ,fast_dev_run=True)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:107: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 227   \n",
      "1 | decoder | Sequential | 240   \n",
      "---------------------------------------\n",
      "467       Trainable params\n",
      "0         Non-trainable params\n",
      "467       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=25). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 6/6 [00:00<00:00,  9.11it/s, loss=0.415, v_num=59]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 6/6 [00:00<00:00,  8.93it/s, loss=0.415, v_num=59]\n"
     ]
    }
   ],
   "source": [
    "finetuner = pl.Trainer(\n",
    "    limit_train_batches=100, max_epochs=5, log_every_n_steps=25\n",
    ")  # ,fast_dev_run=True)\n",
    "finetuner.fit(\n",
    "    model=autoencoder,\n",
    "    train_dataloaders=utils.data.DataLoader(\n",
    "        utils.data.ConcatDataset([test_a_dataset, test_b_dataset]),\n",
    "        batch_size=500,\n",
    "    ), \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1386: UserWarning: `.predict(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.predict(ckpt_path='best')` to use the best model or `.predict(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at /Users/jj/Documents/Projects/kaggle/genentech-404/lightning_logs/version_59/checkpoints/epoch=4-step=30.ckpt\n",
      "Loaded model weights from checkpoint at /Users/jj/Documents/Projects/kaggle/genentech-404/lightning_logs/version_59/checkpoints/epoch=4-step=30.ckpt\n",
      "/anaconda3/envs/kaggle_genentech-404-challenge/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00,  3.54it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[39m=\u001b[39m finetuner\u001b[39m.\u001b[39mpredict(dataloaders\u001b[39m=\u001b[39mtest_a_loader)\n\u001b[1;32m      2\u001b[0m predicts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([result[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m----> 3\u001b[0m predicts[:, \u001b[39m~\u001b[39mmask_categorical] \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39minverse_transform(predicts[:, \u001b[39m~\u001b[39mmask_categorical])\n\u001b[1;32m      5\u001b[0m df_test_a \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mgenentech-404-challenge/test_A.csv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mset_index([\u001b[39m\"\u001b[39m\u001b[39mRID_HASH\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m df_test_a_inferred \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mgenentech-404-challenge/test_A_inferred.csv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mset_index([\u001b[39m\"\u001b[39m\u001b[39mRID_HASH\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "results = finetuner.predict(dataloaders=test_a_loader)\n",
    "predicts = torch.concat([result[1] for result in results], dim=0).numpy()\n",
    "predicts[:, ~mask_categorical] = train_dataset.transform.inverse_transform(predicts[:, ~mask_categorical])\n",
    "\n",
    "df_test_a = pd.read_csv(\"genentech-404-challenge/test_A.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_a_inferred = pd.read_csv(\"genentech-404-challenge/test_A_inferred.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_a_pred = pd.DataFrame(predicts, columns=df_test_a_inferred.columns, index=df_test_a_inferred.index)\n",
    "\n",
    "mask = (~df_test_a_inferred.isna()) & (df_test_a.isna())\n",
    "df_test_a_pred[mask] = df_test_a_inferred[mask]\n",
    "df_test_a_pred.VISCODE = df_test_a.VISCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = finetuner.predict(dataloaders=test_b_loader)\n",
    "predicts = torch.concat([result[0] for result in results], dim=0).numpy()\n",
    "predicts[:, ~mask_categorical] = train_dataset.transform.inverse_transform(predicts[:, ~mask_categorical])\n",
    "\n",
    "df_test_b = pd.read_csv(\"genentech-404-challenge/test_B.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_b_inferred = pd.read_csv(\"genentech-404-challenge/test_B_inferred.csv\").set_index([\"RID_HASH\"])\n",
    "df_test_b_pred = pd.DataFrame(predicts, columns=df_test_b_inferred.columns, index=df_test_b_inferred.index)\n",
    "\n",
    "mask = (~df_test_b_inferred.isna()) & (df_test_b.isna())\n",
    "df_test_b_pred[mask] = df_test_b_inferred[mask]\n",
    "df_test_b_pred.VISCODE = df_test_b.VISCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_df(\n",
    "    ref_df: pd.DataFrame, df_pred_test_a: pd.DataFrame, df_pred_test_b: pd.DataFrame\n",
    "):\n",
    "    df_submit = []\n",
    "    for df, test_type in zip([df_pred_test_a, df_pred_test_b], [\"test_A\", \"test_B\"]):\n",
    "        df_new = pd.melt(\n",
    "            df,\n",
    "            id_vars=[\"RID_HASH\", \"VISCODE\"],\n",
    "            value_vars=[\n",
    "                \"AGE\",\n",
    "                \"PTGENDER_num\",\n",
    "                \"PTEDUCAT\",\n",
    "                \"DX_num\",\n",
    "                \"APOE4\",\n",
    "                \"CDRSB\",\n",
    "                \"MMSE\",\n",
    "                \"ADAS13\",\n",
    "                \"Ventricles\",\n",
    "                \"Hippocampus\",\n",
    "                \"WholeBrain\",\n",
    "                \"Entorhinal\",\n",
    "                \"Fusiform\",\n",
    "                \"MidTemp\",\n",
    "            ],\n",
    "        ).rename(columns={\"value\": \"Predicted\"})\n",
    "        df_new[\"Id\"] = df_new.apply(\n",
    "            lambda x: f\"{x['RID_HASH']}_{x['VISCODE']}_{x['variable']}_{test_type}\",\n",
    "            axis=1,\n",
    "        )\n",
    "        df_submit.append(df_new[[\"Id\", \"Predicted\"]])\n",
    "    df_submit = pd.concat(df_submit).set_index(\"Id\")\n",
    "    return df_submit.loc[ref_df[\"Id\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_sub = pd.read_csv(\"genentech-404-challenge/sample_submission.csv\")\n",
    "df_submission = get_submission_df(\n",
    "    df_sample_sub,\n",
    "    df_test_a_pred.reset_index(),\n",
    "    df_test_b_pred.reset_index(),\n",
    ")\n",
    "df_submission\n",
    "# df_submission.to_csv(\"autoencoder_dropout_11102022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle_genentech-404-challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b1b4e6b55d601fd1eb4b734b7c7b613a61fa9d31c944763ca84ddf1bd43ea2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
